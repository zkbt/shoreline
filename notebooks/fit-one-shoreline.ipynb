{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bb4ba5-937e-4b2b-8659-0c2d4885e992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shoreline import * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80137909",
   "metadata": {},
   "source": [
    "## 📓 Decide which planets and atmospheres we care about. \n",
    "\n",
    "For this notebook, let's focus on the question of whether Solar System objects have any kind of an atmosphere at all. We'll lump together thick H-rich envelopes, substantial CNO-dominated atmospheres, or even microbar atmospheres or cold objects with significant reservoirs of frozen volatiles on their surface. And we'll consider both Solar System planets and exoplanets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de9da89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410aa199",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    subset_and_kind\n",
    "except NameError:\n",
    "    subset_and_kind = 'all-CO2' \\\n",
    "    ''\n",
    "\n",
    "try:\n",
    "    num_warmup\n",
    "    num_samples\n",
    "    num_chains\n",
    "except NameError:\n",
    "    num_warmup=5000\n",
    "    num_samples=50000\n",
    "    num_chains=4\n",
    "\n",
    "try:\n",
    "    uncertainties\n",
    "except NameError:\n",
    "    uncertainties = True\n",
    "\n",
    "print(f\"\"\"\n",
    "🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️\n",
    "🏝️ Let's fit a cosmic shoreline for '{subset_and_kind}' with uncertainties={uncertainties}. 🏝️\n",
    "🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileprefix = f\"{subset_and_kind}-uncertainties={uncertainties}\"\n",
    "import os\n",
    "if os.path.exists(f\"posteriors/{fileprefix}-numpyro.nc\"):\n",
    "    print(f'''\n",
    "    We're skipping a shoreline fit for {subset_and_kind} with uncertainties {uncertainties}\n",
    "    because 'posteriors/{fileprefix}-numpyro.nc' exists.\n",
    "    ''')\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f66e8",
   "metadata": {},
   "source": [
    "## 💨 Assemble planet properties. \n",
    "Let's run a notebook that uses `exoatlas` to assemble some populations of planets and label them with either $\\sf H$ or $\\sf CNO$ atmospheres being present, absent, or unknown. Once it runs, we'll have access to `exoatlas` populations and `astropy` tables with all the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = load_organized_populations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f05dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(A.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b48ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = A[subset_and_kind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_labeled_populations_into_table(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36c981",
   "metadata": {},
   "source": [
    "## Define a shoreline model.\n",
    "\n",
    "Let's write down the equation for our shoreline. \n",
    "- $\\sf f$ is the average relative insolation the planet receives, $\\sf f = L_\\star/4\\pi a^2$\n",
    "- $\\sf v_{esc}$ is the escape velocity at the planet's surface \n",
    "- $\\sf L_\\star$ is the star's bolometric luminosity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636e7b2",
   "metadata": {},
   "source": [
    "Let's say that some fraction $\\sf x$ of the star's luminosity is contributing to atmospheric escape:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b785da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_f_shoreline(log_f_0=0.0, p=0.0, q=0.0, log_v=0, log_L=0):\n",
    "    return log_f_0 + p * log_v + q * log_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a17637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_of_atmosphere(\n",
    "    log_f_0=1.0, p=4.0, q=0.0, ln_w=0, log_v=0, log_L=0, log_f=0\n",
    "):\n",
    "    distance_from_shoreline = log_f - log_f_shoreline(\n",
    "        log_f_0=log_f_0, p=p, q=q, log_v=log_v, log_L=log_L\n",
    "    )\n",
    "    width_of_shoreline = jnp.exp(ln_w)\n",
    "    return 1 / (1 + jnp.exp(distance_from_shoreline / width_of_shoreline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d488f",
   "metadata": {},
   "source": [
    "## Simplify data.\n",
    "\n",
    "Let's transform the data into a simplified table that we can feed into our model, with all quantities (and their uncertainties) already log transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_to_fit = data[['name', 'has_atmosphere']]\n",
    "minimum_uncertainty = 0.01/jnp.log(10)\n",
    "\n",
    "for k in ['relative_escape_velocity', 'relative_insolation', 'stellar_luminosity']:\n",
    "    data_to_fit[f'log_{k}'] = jnp.log10(data[k])\n",
    "    data_to_fit[f'sigma_log_{k}'] = data[f\"{k}_uncertainty\"]/data[k]/jnp.log(10)\n",
    "    uncertainty_is_zero = data_to_fit[f'sigma_log_{k}'] == 0\n",
    "    uncertainty_is_nonzero = data_to_fit[f'sigma_log_{k}'] > 0\n",
    "    uncertainty_is_nonfinite = np.isfinite(data_to_fit[f'sigma_log_{k}']) == False\n",
    "    print(f'''\n",
    "    For {k}:\n",
    "        {sum(uncertainty_is_zero)} with 0 uncertainty\n",
    "        {sum(uncertainty_is_nonzero)} with >0 uncertainty\n",
    "        {sum(uncertainty_is_nonfinite)} with non-finite uncertainty\n",
    "    ''')\n",
    "    data_to_fit[f'sigma_log_{k}'] = jnp.maximum( data_to_fit[f'sigma_log_{k}'], minimum_uncertainty)\n",
    "data_to_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c0e57",
   "metadata": {},
   "source": [
    "## Fit with `numpyro`. \n",
    "Let's define our probabilistic model, with parameters drawn from priors and predictor measurements drawn from their 3D uncertainties. We'll compare predicted probabilities to actual atmosphere labels to define the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cdb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, uncertainties=True):\n",
    "\n",
    "\n",
    "    # set up the four main parameters\n",
    "    log_f_0 = numpyro.sample(\n",
    "        \"log_f_0\", numpyro.distributions.Uniform(-50, 50)\n",
    "    )\n",
    "    p = numpyro.sample(\"p\", numpyro.distributions.Uniform(-50, 50))\n",
    "    q = numpyro.sample(\"q\", numpyro.distributions.Uniform(-50, 50))\n",
    "    ln_w = numpyro.sample(\n",
    "        \"ln_w\",\n",
    "        numpyro.distributions.Uniform(-6, 2),\n",
    "    ) # w = 0.05 to 20\n",
    "    w = numpyro.deterministic('w', jnp.exp(ln_w))\n",
    "\n",
    "    # apply non-informative priors to slopes \n",
    "    numpyro.factor(\"log_prior_for_p\", -1.5 * jnp.log(1 + p * p))\n",
    "    numpyro.factor(\"log_prior_for_q\", -1.5 * jnp.log(1 + q * q))\n",
    "\n",
    "    # extract the labels from the data\n",
    "    has_atmosphere = jnp.array(data[\"has_atmosphere\"]).astype(float)\n",
    "\n",
    "    if uncertainties:\n",
    "        log_v = numpyro.sample('log_v', numpyro.distributions.Normal(data['log_relative_escape_velocity'], data['sigma_log_relative_escape_velocity']))\n",
    "        log_f = numpyro.sample('log_f', numpyro.distributions.Normal(data['log_relative_insolation'], data['sigma_log_relative_insolation']))\n",
    "        log_L = numpyro.sample('log_L', numpyro.distributions.Normal(data['log_stellar_luminosity'], data['sigma_log_stellar_luminosity']))\n",
    "    else:\n",
    "        log_v = data['log_relative_escape_velocity']\n",
    "        log_f = data['log_relative_insolation']\n",
    "        log_L = data['log_stellar_luminosity']\n",
    "   \n",
    "\n",
    "    # make probability predictions based on the model\n",
    "    predicted_probability = probability_of_atmosphere(\n",
    "        log_f_0=log_f_0,\n",
    "        p=p,\n",
    "        q=q,\n",
    "        ln_w=ln_w,\n",
    "        log_v=log_v,\n",
    "        log_f=log_f,\n",
    "        log_L=log_L,\n",
    "    )\n",
    "\n",
    "    # make sure predicted probabilities don't go nan\n",
    "    safe_predicted_probability = jnp.where(\n",
    "        jnp.isnan(predicted_probability), 0, predicted_probability # -jnp.inf, predicted_probability\n",
    "    )\n",
    "    numpyro.sample(\n",
    "        \"has_atmosphere\",\n",
    "        numpyro.distributions.Bernoulli(probs=safe_predicted_probability),\n",
    "        obs=has_atmosphere,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79294e62",
   "metadata": {},
   "source": [
    "Let's sample from the posterior using the No U-Turns Sampler, which is magically good at exploring very high-dimensional probability distributions like ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8aaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = numpyro.infer.NUTS(model)\n",
    "sampler = numpyro.infer.MCMC(\n",
    "    kernel,\n",
    "    num_warmup=num_warmup,\n",
    "    num_samples=num_samples,\n",
    "    num_chains=num_chains,\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    inference = az.from_netcdf(\"posteriors/{fileprefix}-numpyro.nc\")\n",
    "    print('not rerunning, because ')\n",
    "except FileNotFoundError:\n",
    "\n",
    "\n",
    "    key = jax.random.key(42)\n",
    "    key, this_key = jax.random.split(key)\n",
    "    sampler.run(this_key, data=data_to_fit, uncertainties=uncertainties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4fca6a",
   "metadata": {},
   "source": [
    "## See the results. \n",
    "\n",
    "Let's turn the posterior samples from `numpyro` into an `InferenceData` object from `arviz`, which we can use to summarize the results in lots of useful ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ee9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = az.from_numpyro(sampler)\n",
    "inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e083f003",
   "metadata": {},
   "source": [
    "Let's save the posterior samples (excluding the sampled data points, because they take up a lot of space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['log_L', 'log_f', 'log_v']:\n",
    "    try:\n",
    "        del inference.posterior[k]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "mkdir(\"posteriors\")\n",
    "inference.to_netcdf(f\"posteriors/{fileprefix}-numpyro.nc\", groups=\"posterior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f679ae",
   "metadata": {},
   "source": [
    "Let's plot the traces from the independent chains, to make sure they agree and seem reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc43fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [\"log_f_0\", \"p\", \"q\", \"ln_w\"]\n",
    "az.plot_trace(\n",
    "    inference,\n",
    "    var_names=var_names,\n",
    "    backend_kwargs={\"constrained_layout\": True},\n",
    ")\n",
    "plt.suptitle(subset_and_kind)\n",
    "plt.savefig(f\"posteriors/trace-{fileprefix}.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64c839",
   "metadata": {},
   "source": [
    "Let's save a summary of the parameters, with statistics and confidence intervals estimated from samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c085cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.stats import mad_std\n",
    "mad_std\n",
    "\n",
    "\n",
    "\n",
    "func_dict = {\n",
    "    \"mean\": np.mean, \n",
    "    \"std\": np.std,\n",
    "    \"median\": np.median,\n",
    "    \"mad_std\": mad_std, \n",
    "    \"lower\": lambda x: np.median(x) - np.percentile(x, 50-68.3/2), \n",
    "    \"upper\": lambda x: np.percentile(x, 50+68.3/2) - np.median(x) }\n",
    "\n",
    "s = az.summary(inference, var_names=var_names, stat_funcs=func_dict)\n",
    "s.to_csv(f\"posteriors/summary-{fileprefix}.csv\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06cd2b",
   "metadata": {},
   "source": [
    "Let's store a covariance matrix for the parameters (even though many samples might not be well-described by multivariate normal distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix = inference.to_dataframe(groups='posterior')[var_names].cov()\n",
    "covariance_matrix.to_csv(f\"posteriors/covariance-matrix-{fileprefix}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4e4a3",
   "metadata": {},
   "source": [
    "Let's make a rough corner plot to visualize the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "corner.corner(\n",
    "    inference,\n",
    "    var_names=var_names,\n",
    "    fig=fig,\n",
    "    color=\"black\",\n",
    "    hist_kwargs=dict(color=\"black\", density=True),\n",
    "    plot_density=False, plot_datapoints=False, show_titles=True)\n",
    "plt.suptitle(subset_and_kind)\n",
    "plt.savefig(f\"posteriors/corner-{fileprefix}.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exoatlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
