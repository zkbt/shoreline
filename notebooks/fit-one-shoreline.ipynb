{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfb5e63a",
   "metadata": {},
   "source": [
    "# 🌊 Fit one shoreline\n",
    "\n",
    "This notebook chooses one combination of\n",
    "- subset `['all', 'exo', 'solar']` \n",
    "- type of atmosphere `['any', 'CO2']`\n",
    "- whether to include uncertaintes `[True, False]` \n",
    "and fits a cosmic shoreline for that sample.\n",
    "\n",
    "This should be run after `curate-and-label-planets.ipynb`, which is needed to create the organized dataset that will be loaded and fit here. \n",
    "\n",
    "This notebook can be run on its own, or as part of a loop in `fit-many-shorelines.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80137909",
   "metadata": {},
   "source": [
    "## 📓 Set up the fit.\n",
    "\n",
    "To allow this notebook to be run as part of a loop from within another notebook, we use `try/except` statements below to check whether a variable has already been defined (in an enclosing notebook) before defining it ourselves here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dedd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shoreline import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410aa199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what subset and atmosphere kind should we fit? \n",
    "# (any choice of {[all, exo, solar]}-{any, CO2})\n",
    "try:\n",
    "    subset_and_kind\n",
    "except NameError:\n",
    "    subset_and_kind = 'all-CO2'\n",
    "\n",
    "# should we include uncertainties? \n",
    "try:\n",
    "    uncertainties\n",
    "except NameError:\n",
    "    uncertainties = True\n",
    "\n",
    "# set up the sampling parameters\n",
    "try:\n",
    "    num_warmup\n",
    "    num_samples\n",
    "    num_chains\n",
    "except NameError:\n",
    "    num_warmup=5000\n",
    "    num_samples=50000\n",
    "    num_chains=4\n",
    "\n",
    "\n",
    "print(f\"\"\"\n",
    "🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️\n",
    "🏝️ Let's fit a cosmic shoreline for '{subset_and_kind}' with uncertainties={uncertainties}. 🏝️\n",
    "🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️🏝️\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dac55c",
   "metadata": {},
   "source": [
    "If a posterior already exists, let's not run it again. To force it to run again, delete (or move) the posterior `.nc` file. (If being called from within another notebook, these `AssertionError`s will move on to the next fit that needs to be done)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileprefix = f\"{subset_and_kind}-uncertainties={uncertainties}\"\n",
    "import os\n",
    "if os.path.exists(f\"posteriors/{fileprefix}-numpyro.nc\"):\n",
    "    print(f'''\n",
    "    We're skipping a shoreline fit for {subset_and_kind} with uncertainties {uncertainties}\n",
    "    because 'posteriors/{fileprefix}-numpyro.nc' exists.\n",
    "    ''')\n",
    "    assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f66e8",
   "metadata": {},
   "source": [
    "## 💨 Assemble planet properties. \n",
    "Let's load the results of a notebook that used `exoatlas` to assemble some populations of planets and label them with atmospheres being present, absent, or unknown. Once it runs, we'll have access to `exoatlas` populations and `astropy` tables with all the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9f6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = load_organized_populations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f05dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(A.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b48ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = A[subset_and_kind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a9158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = convert_labeled_populations_into_table(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show_in_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36c981",
   "metadata": {},
   "source": [
    "## 🦾 Define a shoreline model.\n",
    "\n",
    "Let's write down the equation for our shoreline. \n",
    "- $\\sf f$ is the average relative insolation the planet receives, $\\sf f = L_\\star/4\\pi a^2$\n",
    "- $\\sf v_{esc}$ is the escape velocity at the planet's surface \n",
    "- $\\sf L_\\star$ is the star's bolometric luminosity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636e7b2",
   "metadata": {},
   "source": [
    "We construct a generative model that tries to explain the atmosphere labels $A_{\\sf i}$ for planet $i$ using the predictors $f_{\\sf i}$, $v_{\\sf esc, i}$, $L_{\\sf \\star, i}$. We first define a cosmic shoreline flux $f_{\\sf shoreline}$ for escape velocity $v_{\\sf esc}$ and stellar luminosity $L_\\star$ with the power law expression\n",
    "\n",
    "$$\\sf \n",
    "f_{\\sf shoreline} = f_{\\sf 0} \\left(\\frac{v_{\\sf esc}}{v_{\\sf esc, \\oplus}}\\right)^p\\left(\\frac{L_\\star}{L_\\odot}\\right)^q\n",
    "$$\n",
    "where $f_{\\sf 0}$, $p$, and $q$ are model parameters, $v_{\\sf esc, \\oplus} = 11.18$ km/s is Earth's escape velocity, and $L_\\odot = 3.828 \\times 10^{26}$ W is the Sun's luminosity. We compare all fluxes to Earth's average bolometric flux $f_\\oplus = L_\\odot/(4\\pi a)^2 = 1361 \\mathrm{W/m^2}$. This power law log transforms to a linear plane\n",
    "$$\\sf \n",
    " \\log_{10} \\left(\\frac{f_{\\sf shoreline}}{f_\\oplus}\\right)  =  \\log_{10} \\left(\\frac{f_{\\sf 0}}{f_\\oplus}\\right)  + p \\cdot  \\log_{10} \\left(\\frac{v_{\\sf esc}}{v_{\\sf esc, \\oplus}}\\right) + q \\cdot  \\log_{10} \\left(\\frac{L_\\star}{L_\\odot}\\right).\n",
    "$$\n",
    "We define a distance from this shoreline in log-flux as \n",
    "$$\\sf\n",
    "\\Delta =  \\log_{10} \\left(\\frac{f_{\\sf }}{f_\\oplus}\\right) - \\log_{10}\\left(\\frac{f_{\\sf shoreline}}{f_\\oplus}\\right) = \\log_{10}\\left(\\frac{f}{f_{\\sf shoreline}}\\right)\n",
    "$$\n",
    "which is similar to the Atmosphere Retention Metric from \\citet{passRecedingCosmicShoreline2025}. We use this distance to describe the probability of each planet having an atmosphere with the logistic function as\n",
    "$$\\sf\n",
    "p_{\\sf i} = P(A_{\\sf i} = 1 | \\mathbf{x}_{\\sf i}, \\boldsymbol{\\theta} ) = \\frac{1}{1+e^{\\Delta_{\\sf i}/w}}\n",
    "$$\n",
    "where $\\mathbf{x}_{\\sf i} = [\\log_{10}(f_{\\sf i}/f_\\oplus), \\log_{10} (v_{\\sf esc, i}/v_{\\sf esc, \\oplus}), \\log_{10} (L_{\\sf \\star, i}/L_\\odot)]$ are the predictors for each datum and $\\boldsymbol{\\theta} = [f_{\\sf 0}, p, q, w]$ are the model parameters. This logistic function smoothly transitions from 1 when $f$ is below the shoreline to 0 above, with the width parameter $w$ describing the fuzziness of the shoreline, how quickly in $\\log_{10} (f/f_\\oplus)$ planets change from mostly having atmospheres to mostly not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b785da82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_f_shoreline(log_f_0=0.0, p=0.0, q=0.0, log_v=0, log_L=0):\n",
    "    return log_f_0 + p * log_v + q * log_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a17637",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_of_atmosphere(\n",
    "    log_f_0=1.0, p=4.0, q=0.0, ln_w=0, log_v=0, log_L=0, log_f=0\n",
    "):\n",
    "    distance_from_shoreline = log_f - log_f_shoreline(\n",
    "        log_f_0=log_f_0, p=p, q=q, log_v=log_v, log_L=log_L\n",
    "    )\n",
    "    width_of_shoreline = jnp.exp(ln_w)\n",
    "    return 1 / (1 + jnp.exp(distance_from_shoreline / width_of_shoreline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793d488f",
   "metadata": {},
   "source": [
    "## 🧹 Simplify data.\n",
    "\n",
    "Let's transform the data into a simplified table that we can feed into our model, with all quantities (and their uncertainties) already log transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6aac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_to_fit = data[['name', 'has_atmosphere']]\n",
    "minimum_uncertainty = 0.01/jnp.log(10)\n",
    "\n",
    "for k in ['relative_escape_velocity', 'relative_insolation', 'stellar_luminosity']:\n",
    "    data_to_fit[f'log_{k}'] = jnp.log10(data[k])\n",
    "    data_to_fit[f'sigma_log_{k}'] = data[f\"{k}_uncertainty\"]/data[k]/jnp.log(10)\n",
    "    uncertainty_is_zero = data_to_fit[f'sigma_log_{k}'] == 0\n",
    "    uncertainty_is_nonzero = data_to_fit[f'sigma_log_{k}'] > 0\n",
    "    uncertainty_is_nonfinite = np.isfinite(data_to_fit[f'sigma_log_{k}']) == False\n",
    "    print(f'''\n",
    "    For {k}:\n",
    "        {sum(uncertainty_is_zero)} with 0 uncertainty\n",
    "        {sum(uncertainty_is_nonzero)} with >0 uncertainty\n",
    "        {sum(uncertainty_is_nonfinite)} with non-finite uncertainty\n",
    "    ''')\n",
    "    data_to_fit[f'sigma_log_{k}'] = jnp.maximum( data_to_fit[f'sigma_log_{k}'], minimum_uncertainty)\n",
    "data_to_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231c0e57",
   "metadata": {},
   "source": [
    "## ☘️ Fit with `numpyro`. \n",
    "Let's define our probabilistic model, with parameters drawn from priors and predictor measurements drawn from their 3D uncertainties. We'll compare predicted probabilities to actual atmosphere labels to define the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62cdb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, uncertainties=True):\n",
    "\n",
    "\n",
    "    # set up the four main parameters\n",
    "    log_f_0 = numpyro.sample(\n",
    "        \"log_f_0\", numpyro.distributions.Uniform(-50, 50)\n",
    "    )\n",
    "    p = numpyro.sample(\"p\", numpyro.distributions.Uniform(-50, 50))\n",
    "    q = numpyro.sample(\"q\", numpyro.distributions.Uniform(-50, 50))\n",
    "    ln_w = numpyro.sample(\n",
    "        \"ln_w\",\n",
    "        numpyro.distributions.Uniform(-6, 2),\n",
    "    ) # w = 0.05 to 20\n",
    "    w = numpyro.deterministic('w', jnp.exp(ln_w))\n",
    "\n",
    "    # apply non-informative priors to slopes \n",
    "    numpyro.factor(\"log_prior_for_p\", -1.5 * jnp.log(1 + p * p))\n",
    "    numpyro.factor(\"log_prior_for_q\", -1.5 * jnp.log(1 + q * q))\n",
    "\n",
    "    # extract the labels from the data\n",
    "    has_atmosphere = jnp.array(data[\"has_atmosphere\"]).astype(float)\n",
    "\n",
    "    if uncertainties:\n",
    "        log_v = numpyro.sample('log_v', numpyro.distributions.Normal(data['log_relative_escape_velocity'], data['sigma_log_relative_escape_velocity']))\n",
    "        log_f = numpyro.sample('log_f', numpyro.distributions.Normal(data['log_relative_insolation'], data['sigma_log_relative_insolation']))\n",
    "        log_L = numpyro.sample('log_L', numpyro.distributions.Normal(data['log_stellar_luminosity'], data['sigma_log_stellar_luminosity']))\n",
    "    else:\n",
    "        log_v = data['log_relative_escape_velocity']\n",
    "        log_f = data['log_relative_insolation']\n",
    "        log_L = data['log_stellar_luminosity']\n",
    "   \n",
    "\n",
    "    # make probability predictions based on the model\n",
    "    predicted_probability = probability_of_atmosphere(\n",
    "        log_f_0=log_f_0,\n",
    "        p=p,\n",
    "        q=q,\n",
    "        ln_w=ln_w,\n",
    "        log_v=log_v,\n",
    "        log_f=log_f,\n",
    "        log_L=log_L,\n",
    "    )\n",
    "\n",
    "    # make sure predicted probabilities don't go nan\n",
    "    safe_predicted_probability = jnp.where(\n",
    "        jnp.isnan(predicted_probability), 0, predicted_probability # -jnp.inf, predicted_probability\n",
    "    )\n",
    "    numpyro.sample(\n",
    "        \"has_atmosphere\",\n",
    "        numpyro.distributions.Bernoulli(probs=safe_predicted_probability),\n",
    "        obs=has_atmosphere,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79294e62",
   "metadata": {},
   "source": [
    "Let's sample from the posterior using the No U-Turns Sampler, which is magically good at exploring very high-dimensional probability distributions like ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8aaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = numpyro.infer.NUTS(model)\n",
    "sampler = numpyro.infer.MCMC(\n",
    "    kernel,\n",
    "    num_warmup=num_warmup,\n",
    "    num_samples=num_samples,\n",
    "    num_chains=num_chains,\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "try:\n",
    "    inference = az.from_netcdf(\"posteriors/{fileprefix}-numpyro.nc\")\n",
    "    print('not rerunning, because ')\n",
    "except FileNotFoundError:\n",
    "\n",
    "\n",
    "    key = jax.random.key(11)\n",
    "    key, this_key = jax.random.split(key)\n",
    "    sampler.run(this_key, data=data_to_fit, uncertainties=uncertainties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4fca6a",
   "metadata": {},
   "source": [
    "## 🎨 See the results. \n",
    "\n",
    "Let's turn the posterior samples from `numpyro` into an `InferenceData` object from `arviz`, which we can use to summarize the results in lots of useful ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ee9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference = az.from_numpyro(sampler)\n",
    "inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e083f003",
   "metadata": {},
   "source": [
    "Let's save the posterior samples (excluding the sampled data points, because they take up a lot of space)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce5803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['log_L', 'log_f', 'log_v']:\n",
    "    try:\n",
    "        del inference.posterior[k]\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "mkdir(\"posteriors\")\n",
    "inference.to_netcdf(f\"posteriors/{fileprefix}-numpyro.nc\", groups=\"posterior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f679ae",
   "metadata": {},
   "source": [
    "Let's plot the traces from the independent chains, to make sure they agree and seem reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc43fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = [\"log_f_0\", \"p\", \"q\", \"ln_w\"]\n",
    "az.plot_trace(\n",
    "    inference,\n",
    "    var_names=var_names,\n",
    "    backend_kwargs={\"constrained_layout\": True},\n",
    ")\n",
    "plt.suptitle(subset_and_kind)\n",
    "plt.savefig(f\"posteriors/trace-{fileprefix}.pdf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c64c839",
   "metadata": {},
   "source": [
    "Let's save a summary of the parameters, with statistics and confidence intervals estimated from samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c085cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.stats import mad_std\n",
    "mad_std\n",
    "\n",
    "\n",
    "\n",
    "func_dict = {\n",
    "    \"mean\": np.mean, \n",
    "    \"std\": np.std,\n",
    "    \"median\": np.median,\n",
    "    \"mad_std\": mad_std, \n",
    "    \"lower\": lambda x: np.median(x) - np.percentile(x, 50-68.3/2), \n",
    "    \"upper\": lambda x: np.percentile(x, 50+68.3/2) - np.median(x) }\n",
    "\n",
    "s = az.summary(inference, var_names=var_names, stat_funcs=func_dict)\n",
    "s.to_csv(f\"posteriors/summary-{fileprefix}.csv\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06cd2b",
   "metadata": {},
   "source": [
    "Let's store a covariance matrix for the parameters (even though many samples might not be well-described by multivariate normal distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce0069",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix = inference.to_dataframe(groups='posterior')[var_names].cov()\n",
    "covariance_matrix.to_csv(f\"posteriors/covariance-matrix-{fileprefix}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea4e4a3",
   "metadata": {},
   "source": [
    "Let's make a rough corner plot to visualize the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031d03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "corner.corner(\n",
    "    inference,\n",
    "    var_names=var_names,\n",
    "    fig=fig,\n",
    "    color=\"black\",\n",
    "    hist_kwargs=dict(color=\"black\", density=True),\n",
    "    plot_density=False, plot_datapoints=False, show_titles=True)\n",
    "plt.suptitle(subset_and_kind)\n",
    "plt.savefig(f\"posteriors/corner-{fileprefix}.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exoatlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
