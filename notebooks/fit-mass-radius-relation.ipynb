{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6275ffa2",
   "metadata": {},
   "source": [
    "# 🌎⚖️📏 Fit a Mass-Radius Relation\n",
    "\n",
    "This notebook fits for a mass-radius relationshhip with an intrinstic scatter, incorporating uncertainties in both dimensions. Since some planets have interesting/useful atmosphere constraints worth including in a cosmic shoreline (LHS 3844b, some small Solar System bodies) but no measured masses, we can use the mass radius relation derived here to estimate their masses and (cautious) uncertainties on those masses. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a32d43-3116-4668-aeaa-7a7068ebc8a2",
   "metadata": {},
   "source": [
    "## ☄️🌕🌎🪐 Load planet data.\n",
    "\n",
    "We'll use [`exoatlas`](https://zkbt.github.io/exoatlas/) to download some catalogs of exoplanet and Solar System properties. The first time this runs it might be a little slow because it needs to download big tables from the NASA Exoplanet Archive, but from then onward it should be a bit faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22d0b0-85b1-4ea2-9229-8597aaf73944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shoreline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a558c-e35d-4036-bd17-c9b706249853",
   "metadata": {},
   "outputs": [],
   "source": [
    "exoplanets = TransitingExoplanets()\n",
    "major = SolarSystem()\n",
    "dwarf = SolarSystemDwarfPlanets()\n",
    "moons = SolarSystemMoons()\n",
    "minor = SolarSystemMinorPlanets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abba9c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "planets = [] \n",
    "for p in [exoplanets, major, dwarf, moons, minor]:\n",
    "    ok = (p.get_fractional_uncertainty('mass') < 0.25) & (p.get_fractional_uncertainty('radius') < 0.25)\n",
    "    planets.append(p[ok])\n",
    "planets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287313c8-238d-4f30-b8b9-e8165839d98d",
   "metadata": {},
   "source": [
    "Let's make a quick plot to understand what data we're working with. We'll use some built-in [`Plottable` objects from `exoatlas`](https://zkbt.github.io/exoatlas/user/visualizing/) to visualize these planets togehther."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff9fd5-90b2-445b-a6d9-d49c7b6c7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exoatlas.visualizations import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090a5ba8-0149-45b3-a013-e47c36e6c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "major.annotate_planets = True\n",
    "g = GridGallery(rows=[EscapeVelocity(lim=[1e-3, 4e2]), Mass(lim=[1e-10, 3e3])], \n",
    "                cols=[Radius(lim=[4e-4, 3e1])], map_type=ErrorMap)\n",
    "g.build(planets)\n",
    "list(g.maps.values())[0].add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b110c-6153-4ea0-8a9d-dd01edbd5529",
   "metadata": {},
   "source": [
    "## 💾 Curate the data. \n",
    "\n",
    "Now, let's extract one big [`astropy.table`](https://docs.astropy.org/en/stable/table/index.html) of quantities we'll consider in our fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f6b8e3-de28-4e5b-a4bf-6b680e02c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in planets:\n",
    "    p.add_column('type', p.label)\n",
    "data = vstack([p.create_table(['name', 'type', 'radius', 'radius_uncertainty', 'mass', 'mass_uncertainty']) for p in planets])\n",
    "\n",
    "# replace 0 uncertainties with 1% (but keep nan values)\n",
    "for k in ['radius', 'mass']:\n",
    "    i = (data[f'{k}_uncertainty'] == 0) | np.isnan(data[f'{k}_uncertainty'])\n",
    "    data[f'{k}_uncertainty'][i] = data[f'{k}'][i]*0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26412b-90da-4bed-8009-942bbea42f75",
   "metadata": {},
   "source": [
    "We want to fit for a power-law relation $M \\propto R^m$. Practically, it's easier to implement that as a straight line in log-transformed data. Let's define our axes, and set up $\\sf x, y, \\sigma_x, \\sigma_y$. \n",
    "\n",
    "$$\\sf x,i = \\ln R_i$$\n",
    "$$\\sf y,i = \\ln M_i$$\n",
    "$$\\sf \\sigma_{x,i} = \\frac{\\sigma_{R,i}}{R_i}$$\n",
    "$$\\sf \\sigma_{y,i} = \\frac{\\sigma_{M,i}}{M_i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1acfa9e",
   "metadata": {},
   "source": [
    "Let's extract the data we need, log-transform it, estimate log-transformed uncertainties, and plot them to make sure they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d571c-c2cb-49e0-8551-77b3c8bc2681",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_x, k_y = 'radius', 'mass'\n",
    "rocky_radius_cutoff = 1.8*u.Rearth\n",
    "ok = data['radius'] < rocky_radius_cutoff \n",
    "\n",
    "for k in [k_x, k_y]:\n",
    "    for suffix in ['', '_uncertainty']:\n",
    "        ok *= np.isfinite(data[f'{k}{suffix}'])\n",
    "        ok *= data[f'{k}{suffix}']>0\n",
    "    fractional_uncertainty = data[f'{k}_uncertainty']/data[k]\n",
    "    #ok *= fractional_uncertainty < 0.25\n",
    "    data[f'{k}_uncertainty'] = np.maximum(data[f'{k}_uncertainty'], 0.01*data[k])\n",
    "    \n",
    "subset = data[ok]\n",
    "x = jnp.log(subset[f'{k_x}'].value)\n",
    "y = jnp.log(subset[f'{k_y}'].value)\n",
    "sigma_x = jnp.array(subset[f'{k_x}_uncertainty'].value/subset[f'{k_x}'].value)\n",
    "sigma_y = jnp.array(subset[f'{k_y}_uncertainty'].value/subset[f'{k_y}'].value)\n",
    "\n",
    "# make (approximate) covariance matrices for each data point\n",
    "N = len(subset)\n",
    "S = np.zeros((N, 2, 2))\n",
    "S[:,0,0] = sigma_x**2\n",
    "S[:,1,1] = sigma_y**2\n",
    "S = jnp.array(S)\n",
    "    \n",
    "plt.figure(constrained_layout=True)\n",
    "ekw = dict(linewidth=0, elinewidth=1)\n",
    "\n",
    "for t in np.unique(subset['type']):\n",
    "    i = subset['type'] == t\n",
    "    plt.errorbar(x[i], y[i],  xerr=sigma_x[i], yerr=sigma_y[i], label=t, **ekw)\n",
    "    plt.xlabel(f'x = ln({k_x})'); plt.ylabel('y = ln({k_y})')\n",
    "plt.legend(frameon=False, bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aea734c-6bbb-4932-bc57-f46923f4d258",
   "metadata": {},
   "source": [
    "## 🧑‍🎨 Define a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94b6b1d-5e05-4c7e-b512-728d40c3e3d6",
   "metadata": {},
   "source": [
    "Let's follow [this outline from D. Foreman-Mackey](https://dfm.io/posts/fitting-a-plane/), with some extra context from [D. Hogg et al. (2010)](https://ui.adsabs.harvard.edu/abs/2010arXiv1008.4686H/abstract) and [J. Vanderplas (2014)](https://ui.adsabs.harvard.edu/abs/2014arXiv1411.5018V/abstract) (see also [this blog post](https://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/#Prior-on-Slope-and-Intercept)). We'll be fitting a model that looks like \n",
    "\n",
    "$$\\sf y = m\\cdot x + b = m \\cdot ln R + b$$\n",
    "\n",
    "with an instrinsic scatter in the y direction (beyond the measurement uncertainties of any particular planet) of \n",
    "\n",
    "$$\\sf ln \\sigma_{y} = m_\\sigma \\cdot x + b_\\sigma =  m_\\sigma\\cdot ln R + b_\\sigma  $$\n",
    "\n",
    "As explained into \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848fe3c-23b4-4f97-8ae9-19beebe458db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "068fc000-50b9-4a60-a569-75da13ecae10",
   "metadata": {},
   "source": [
    "## ⏳ Infer the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f077c9-3652-4285-868b-1c3ae0e50f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "\n",
    "\n",
    "def model(x, y, S):\n",
    "    m = numpyro.sample(\"m\", numpyro.distributions.Normal(0, 10))\n",
    "    b = numpyro.sample(\"b\", numpyro.distributions.Normal(0, 10))\n",
    "    m_sigma = numpyro.sample(\"m_sigma\", numpyro.distributions.Normal(0, 10))\n",
    "    b_sigma = numpyro.sample(\"b_sigma\", numpyro.distributions.Normal(0, 10))\n",
    "    log_sigma = m_sigma * x + b_sigma\n",
    "    instrinsic_scatter = jnp.exp(log_sigma)\n",
    "\n",
    "    # a vector pointing perpendicular to the line\n",
    "    v = jnp.array([-m, 1.0])\n",
    "\n",
    "    # the effective error in the direction between the line and the data point\n",
    "    Sigma2 = jnp.dot(jnp.dot(S, v), v) + instrinsic_scatter**2\n",
    "\n",
    "    # the offset in the y direction\n",
    "    Delta = m * x + b - y\n",
    "\n",
    "    # the product of all the individual likelihoods\n",
    "    log_likelihood = -0.5 * jnp.sum(Delta**2 / Sigma2 + jnp.log(Sigma2))\n",
    "    numpyro.factor(\"log_likelihood\", log_likelihood)\n",
    "\n",
    "    # a non-informative prior to avoid large prior space near infinite slope\n",
    "    numpyro.factor(\"log_prior_for_m\", -1.5 * jnp.log(1 + m**2))\n",
    "    numpyro.factor(\"log_prior_for_m_sigma\", -1.5 * jnp.log(1 + m_sigma**2))\n",
    "\n",
    "\n",
    "import arviz as az\n",
    "import jax\n",
    "import corner\n",
    "\n",
    "\n",
    "# sample from the prior\n",
    "n_prior_samples = 5000\n",
    "key = jax.random.key(0)\n",
    "prior_samples = numpyro.infer.Predictive(model, num_samples=n_prior_samples)(\n",
    "    key, x, y, S\n",
    ")\n",
    "converted_prior_samples = {\n",
    "    f\"{p}\": np.expand_dims(prior_samples[p], axis=0) for p in prior_samples\n",
    "}\n",
    "prior_samples_inference = az.from_dict(converted_prior_samples)\n",
    "\n",
    "az.summary(prior_samples_inference)\n",
    "\n",
    "# sample from the posterior\n",
    "sampler = numpyro.infer.MCMC(\n",
    "    numpyro.infer.NUTS(\n",
    "        model,\n",
    "    ),\n",
    "    num_warmup=5000,\n",
    "    num_samples=50000\n",
    "    ,\n",
    "    num_chains=4,\n",
    "    progress_bar=True,\n",
    ")\n",
    "sampler.run(jax.random.key(1), x, y, S)\n",
    "inference = az.from_numpyro(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f8d36-f2bd-40ec-8163-d4b1c078cb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names=['m', 'b','m_sigma', 'b_sigma']\n",
    "az.summary(inference, var_names=var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13119d60-436d-4386-b4e5-dbfc5a4f3563",
   "metadata": {},
   "source": [
    "## 🎨 Visualize the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5859cb12-8eb7-4e22-b356-0a6a335c33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(prior_samples_inference, colors='black', var_names=var_names)\n",
    "fig = corner.corner(inference,  color='red', fig=fig, var_names=var_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83907ae-b51b-4ad5-b7db-448fe899fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = corner.corner(inference,  color='red', var_names=var_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f23365f-2fbf-4c39-804a-d765e798e264",
   "metadata": {},
   "source": [
    "## 🖋️ Plot the new mass-radius relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75014fd9-08ee-4e2f-8c1e-d02b9cc642fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi, ax = plt.subplots(1, 2, constrained_layout=True, figsize=(9,4))\n",
    "\n",
    "e = ErrorMap(xaxis=Radius(lim=[4e-4, 3e1]), yaxis=Mass(lim=[1e-10, 3e3]))\n",
    "e.build(planets, ax=ax[0])\n",
    "e.add_legend(loc='upper left', fontsize=7)\n",
    "\n",
    "z = ErrorMap(xaxis=Radius(lim=[.3, 4.5]), yaxis=Mass(lim=[.03, 30]))\n",
    "z.build(planets, ax=ax[1])\n",
    "\n",
    "x_smooth = np.linspace(np.log(1e-4), np.log(rocky_radius_cutoff.to_value(u.Rearth)))\n",
    "\n",
    "N = 20\n",
    "samples = az.extract(inference, num_samples=N)\n",
    "\n",
    "for a in ax:\n",
    "    plt.sca(a)\n",
    "    for i in range(N):\n",
    "        b = samples.b.values[i]\n",
    "        m = samples.m.values[i]\n",
    "        y_center = m*x_smooth + b\n",
    "    \n",
    "        m_sigma = samples.m_sigma.values[i]\n",
    "        b_sigma =  samples.b_sigma.values[i]\n",
    "        log_sigma = m_sigma*x_smooth + b_sigma\n",
    "        delta_y = np.exp(log_sigma)\n",
    "        plt.fill_between(np.exp(x_smooth), np.exp(y_center+delta_y), np.exp(y_center-delta_y), linewidth=0, alpha=0.02, color='orangered', zorder=10)\n",
    "        #plt.plot(np.exp(x_smooth), np.exp(m*x_smooth + b), color='orangered', alpha=0.02)\n",
    "ax[0].indicate_inset_zoom(ax[1])\n",
    "plt.sca(ax[1])\n",
    "\n",
    "plt.text(0.97, 0.03, f'$\\sf ln (M/M_\\oplus) = {m:.3f} \\cdot ln (R/R_\\oplus) {b:+.3f}$\\n(valid only for R<{rocky_radius_cutoff.to_string(format=\"latex_inline\")})', \n",
    "         transform=ax[1].transAxes, fontsize=8,\n",
    "         va = 'bottom', ha='right', color='orangered')# $\\n$\\sf ln \\sigma_{{ln M}} =  m_\\sigma\\cdot ln R + b_\\sigma$'\n",
    "\n",
    "\n",
    "\n",
    "plt.savefig('figures/mass-radius-relation-for-rocky-planets.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24d0175-d192-49bd-842d-806de5d1127a",
   "metadata": {},
   "source": [
    "## 🥏 Print parameters and covariance matrix.\n",
    "\n",
    "We'd like to use the parameters we inferred from this fit to estimate the masses of other planets. Let's print out a snippet of code that we can paste into another code to represent both the mean parameters $\\mu$ and their covariance matrix $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dfb5b0-f8ec-4e65-88cc-1412c7a6e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(inference, var_names=var_names)#, stat_focus='median')\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dcdae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in summary['mean'].keys():\n",
    "    print(f'{k} = {summary[\"mean\"][k]} \\pm {summary[\"sd\"][k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e15e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = summary['mean']\n",
    "mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763cb441",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_sigma = summary['mean']['b_sigma']\n",
    "m_sigma = summary['mean']['m_sigma']\n",
    "\n",
    "R = np.array([1, 1e-3])\n",
    "np.exp(np.log(R)*m_sigma + b_sigma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee692a3-ab90-4a82-9f38-840227e4ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_matrix = inference.posterior.to_dataframe()[var_names].cov()\n",
    "covariance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d253896-5e08-41ba-9848-a2ee9763791b",
   "metadata": {},
   "source": [
    "Let's print out some arrays that can be pasted into a mass-radius estimator in `exoatlas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ae824-0d07-4c80-90ea-e306d2aa95de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''\n",
    "mu = np.{repr(np.array(mu))}\n",
    "{', '.join(var_names)} = mu\n",
    "covariance_matrix = np.{repr(np.array(covariance_matrix))}\n",
    "rocky_radius_cutoff={rocky_radius_cutoff.to_value('R_earth')}*u.Rearth\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0bfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_dict = {\n",
    "    \"median\": np.median,\n",
    "    \"lower\": lambda x: np.median(x) - np.percentile(x, 50-68.3/2), \n",
    "    \"upper\": lambda x: np.percentile(x, 50+68.3/2) - np.median(x) }\n",
    "\n",
    "\n",
    "def create_latex_name(s):\n",
    "    return clean(s).replace('0', 'o')\n",
    "\n",
    "po = inference.posterior\n",
    "po['scatter-at-earth'] = np.exp(po['b_sigma'])*100\n",
    "po['scatter-at-small'] = np.exp(po['b_sigma'] + po['m_sigma']*np.log(1e-3))*100\n",
    "\n",
    "\n",
    "po['power-R-vesc'] = 2/(po['m'] - 1)\n",
    "po['power-M-vesc'] = 2*po['m']/(po['m'] - 1)\n",
    "po['power-p'] = 4 - po['power-R-vesc']\n",
    "\n",
    "\n",
    "with open('posteriors-mass-radius.tex', 'w') as f:\n",
    "    f.write('% mass-radius\\n')\n",
    "    summary = az.summary(inference,  stat_funcs=func_dict)\n",
    "    for k in summary['median'].keys():\n",
    "        lower, upper = summary[\"lower\"][k], summary[\"upper\"][k] \n",
    "        symmetric = np.abs(lower - upper)/(lower + upper) < 0.1\n",
    "        if symmetric:\n",
    "            sigma = (lower + upper)/2\n",
    "            s = f'{summary[\"median\"][k]:.3g} \\pm {sigma:.2g}'\n",
    "        else:\n",
    "            s = f'{summary[\"median\"][k]:.3g}_{{-{lower:.2g}}}^{{+{upper:.2g}}}'\n",
    "        f.write(rf'\\newcommand{{\\{'mr'+create_latex_name(k)}}}{{{s}}}' + '\\n')\n",
    "        s = f'{summary[\"median\"][k]:.3g}'\n",
    "        f.write(rf'\\newcommand{{\\{'mr'+create_latex_name(k)+'justvalue'}}}{{{s}}}' + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d717a5-5bb0-4dd7-9809-0b32fd57a25b",
   "metadata": {},
   "source": [
    "Hooray, we've done it! Let's go on to try to fit a shoreline!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exoatlas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
